/*
 *   Copyright (c) 2024 Arcology Network

 *   This program is free software: you can redistribute it and/or modify
 *   it under the terms of the GNU General Public License as published by
 *   the Free Software Foundation, either version 3 of the License, or
 *   (at your option) any later version.

 *   This program is distributed in the hope that it will be useful,
 *   but WITHOUT ANY WARRANTY; without even the implied warranty of
 *   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
 *   GNU General Public License for more details.

 *   You should have received a copy of the GNU General Public License
 *   along with this program.  If not, see <https://www.gnu.org/licenses/>.
 */

package eu

import (
	"errors"

	"github.com/arcology-network/common-lib/common"
	statecell "github.com/arcology-network/common-lib/crdt/statecell"
	mapi "github.com/arcology-network/common-lib/exp/map"
	slice "github.com/arcology-network/common-lib/exp/slice"
	eucommon "github.com/arcology-network/eu/common"
	ethadaptor "github.com/arcology-network/eu/ethadaptor"
	euintf "github.com/arcology-network/eu/interface"
	arbitrator "github.com/arcology-network/scheduler/arbitrator"
	workload "github.com/arcology-network/scheduler/workload"
	stgcommon "github.com/arcology-network/state-engine/common"
	"github.com/arcology-network/state-engine/state/cache"
	evmcommon "github.com/ethereum/go-ethereum/common"
	"github.com/ethereum/go-ethereum/core/vm"
)

// ExecutionPipeline coordinates the full multi-stage execution flow for a workload
// generation. It manages parallel execution of job sequences, sequential execution
// of jobs within each sequence, state-snapshot creation via API cascades, merging
// of per-job state changes, conflict detection across sequences, and transition
// normalization (gas/nonce). This struct provides the top-level orchestration for
// Arcologyâ€™s deterministic parallel EVM execution model.
type ExecutionPipeline struct {
	NumThreads uint32           // Maximum parallelism for running job sequences.
	Config     *eucommon.Config // Runtime/block-level configuration (dynamic context).
}

// The run function executes the job sequences in parallel and returns the results in a single slice.
// The blockAPI is used to access the state data. For external transaction execution, the blockAPI has
// all the state data from the last block. For the spawned transaction execution, the blockAPI has the state data
// of it parent thread up to the point of the point of the thread creation. The child thread uses the state data of the parent
// thread to create a state snapshot for itself. Eventually, all the state changes generated by the child threads will be
// merged back into the parent thread.
//
// But when a child thread is trying to deploy a contract, it needs to increment the nonce of the caller contract and
// the nonce is a global counter for the account. Since there is no inter-thread communication, the child will increment
// the nonce of the parent thread by itself independently. Different child threads may deploy their contracts at the same address.

// This isn't a problem for the external transaction execution, the conflict detector will find it out and revert the transactions.
// But for the spawned transaction execution, sometimes we need to deploy some temporary contracts to do their jobs, and certainly we
// don't want to cause any conflict. That is why we need to give different nonceOffset to different child threads, so they can deploy
// their contracts at different addresses.

func (this *ExecutionPipeline) RunGeneration(generation *workload.Generation, blockAPI euintf.EthApiRouter) []*statecell.StateCell {
	seqIDs := make([][]uint64, len(generation.JobSeqs))
	records := make([][]*statecell.StateCell, len(generation.JobSeqs))

	// Execute the job sequences in parallel. All the access records from the same sequence share
	// the same sequence ID. The sequence ID is used to detect the conflicts between different sequences.
	slice.ParallelForeach(generation.JobSeqs, int(this.NumThreads), func(i int, _ **workload.JobSequence) {
		seqIDs[i], records[i] = this.RunSequence(generation.JobSeqs[i], blockAPI.Cascade(), uint64(i))
	})

	conflictInfo := this.DetectConflicts(seqIDs, records)
	txDict, seqDict, _ := conflictInfo.ToDict()

	// Mark the conflicts in the job sequences.
	cleanTrans := slice.Concate(generation.JobSeqs, func(seq *workload.JobSequence) []*statecell.StateCell {
		if _, ok := seqDict[(*seq).ID]; ok { // Check if the sequence ID is in the conflict list.
			(*seq).FlagConflict(txDict, errors.New(stgcommon.WARN_ACCESS_CONFLICT))
		}
		return (*seq).GetClearedTransition() // Return the conflict-free transitions
	})
	return cleanTrans
}

// There needs to be a sequence ID for each transaction in the sequence, not just the transaction ID because
// multiple transactions may be in the same sequence and they may have the same transaction ID.
func (this *ExecutionPipeline) DetectConflicts(seqIDs [][]uint64, records [][]*statecell.StateCell) arbitrator.Conflicts {
	if len(records) == 1 {
		return arbitrator.Conflicts{}
	}
	return arbitrator.Conflicts(arbitrator.NewArbitrator().InsertAndDetect(slice.Flatten(seqIDs), slice.Flatten(records)))
}

// Execute a sequence of jobs in a sequential order.
func (this *ExecutionPipeline) RunSequence(jobSeq *workload.JobSequence, seqAPI euintf.EthApiRouter, threadId uint64) ([]uint64, []*statecell.StateCell) {
	// seqAPI = seqAPI //.Cascade() // Create a new write cache for the sequence with the main router as the data source.
	seqAPI.DecrementDepth()

	// Only one transaction in the sequence, no need to create a new API router.
	if len(jobSeq.Jobs) == 1 {
		this.RunJob(jobSeq.Jobs[0], this.Config, seqAPI.Cascade())
		return slice.Fill(make([]uint64, len(jobSeq.Jobs[0].Result.RawStateAccesses)), jobSeq.ID), jobSeq.Jobs[0].Result.RawStateAccesses
	}

	// Multiple transactions in the sequence, execute them one by one.
	for _, job := range jobSeq.Jobs {
		txApi := seqAPI.Cascade() // A new router whose writeCache uses the parent APIHandler's writeCache as the data source.
		txApi.DecrementDepth()    // The api router always increments the depth.  So we need to decrement it here.

		this.RunJob(job, this.Config, txApi)

		// the line below modifies the cache in the major api as well.
		seqAPI.StateCache().(*cache.StateCache).Insert(job.Result.RawStateAccesses) // Merge the txApi write cache back into the api router.
		mapi.Merge(txApi.AuxDict(), seqAPI.AuxDict())                               // The tx may generate new aux data, so merge it into the main api router.
	}

	// Get acumulated state access records from all the transactions in the sequence.
	accmulatedAccessRecords := statecell.StateCells(seqAPI.StateCache().(*cache.StateCache).Export()).To(statecell.IPAccess{})
	return slice.Fill(make([]uint64, len(accmulatedAccessRecords)), jobSeq.ID), accmulatedAccessRecords
}

func (this *ExecutionPipeline) RunJob(job *workload.Job, configInfo *eucommon.Config, api euintf.EthApiRouter) {
	statedb := ethadaptor.NewImplStateDB(api)
	statedb.PrepareFormer(job.StdMsg.TxHash, [32]byte{}, uint64(job.StdMsg.ID))

	eu := NewEU(
		configInfo.ChainConfig,
		vm.Config{},
		statedb,
		api,
	)

	receipt, evmResult, prechkErr :=
		eu.Run(
			job,
			eucommon.NewEVMBlockContext(configInfo),
			eucommon.NewEVMTxContext(*job.StdMsg.Native),
		)

	job.Result = (&workload.Result{
		TxIndex:          uint64(job.StdMsg.ID),
		TxHash:           common.IfThenDo1st(receipt != nil, func() evmcommon.Hash { return receipt.TxHash }, evmcommon.Hash{}),
		RawStateAccesses: eu.GetStateAccesses(),
		Err:              common.IfThenDo1st(prechkErr == nil, func() error { return evmResult.Err }, prechkErr),
		Receipt:          receipt,
		EvmResult:        evmResult,
		StdMsg:           job.StdMsg,
	})

	normalizer := statecell.NewTransactionNormalizer(job.Result.Receipt.GasUsed, *configInfo.Coinbase, job.StdMsg)
	job.Result.Immuned = normalizer.Normalize(job.Result.Transitions())
}
